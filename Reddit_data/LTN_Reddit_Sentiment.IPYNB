{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1002fac3",
   "metadata": {},
   "source": [
    "# Reddit Sentiment Anlysis: \n",
    "## London low-traffic Neighborhood zones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466cc628",
   "metadata": {},
   "source": [
    "1. Data \"Aquisition\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce994ab4",
   "metadata": {},
   "source": [
    "Reddit r/London scraping script\n",
    "- accesses json endpoint \n",
    "- looks for given keywords \n",
    "- gets given number of posts + 5 comments each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote \n",
    "\n",
    "URL = \"https://www.reddit.com/r/london\"\n",
    "keywords = [\"low traffic neighbourhood\", \"low-traffic neighbourhood\", \"LTN\", \"low-traffic zones\"]\n",
    "Headers = {'User-Agent': 'LTN_Sentiment_Analysis (University of Amsterdam)'}\n",
    "\n",
    "results = []\n",
    "seen_posts = set()\n",
    "\n",
    "def search_keyword(keyword):\n",
    "    url = f\"{URL}/search.json?q={quote(keyword)}&restrict_sr=1&sort=relevance&limit=100&t=all\"\n",
    "    return requests.get(url, headers=Headers).json()['data']['children']\n",
    "\n",
    "def get_comments(post_id):\n",
    "    url = f\"{URL}/comments/{post_id}.json\"\n",
    "    data = requests.get(url, headers=Headers).json()\n",
    "    return data[1]['data']['children'] if len(data) >= 2 else []\n",
    "\n",
    "def process_comments(comment_data, post_id, post_title, max_comments=5):\n",
    "    comments = []\n",
    "    for item in comment_data:\n",
    "        if len(comments) >= max_comments:\n",
    "            break\n",
    "        if item.get('kind') == 't1':\n",
    "            comment = item['data']\n",
    "            body = comment.get('body', '')\n",
    "            if body and body not in ['[removed]', '[deleted]']:\n",
    "                comments.append({\n",
    "                    'type': 'comment',\n",
    "                    'title': '',\n",
    "                    'created_utc': datetime.fromtimestamp(comment.get('created_utc', 0)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'body': body,\n",
    "                    'num_comments': '',\n",
    "                    'url': f\"{URL}{comment.get('permalink', '')}\",\n",
    "                    'keyword_matched': '',\n",
    "                    'parent_post_title': post_title,\n",
    "                })\n",
    "            if len(comments) < max_comments:\n",
    "                replies = comment.get('replies', {})\n",
    "                if isinstance(replies, dict):\n",
    "                    comments.extend(process_comments(replies['data']['children'], post_id, post_title, max_comments - len(comments)))\n",
    "    return comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9c288",
   "metadata": {},
   "source": [
    "Nice Progress Bar by chat gpt :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564c5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable progress bar function\n",
    "def print_progress_bar(current, total, bar_length=40, label=\"Progress\"):\n",
    "    percent = current / total\n",
    "    filled = int(bar_length * percent)\n",
    "    bar = '█' * filled + '░' * (bar_length - filled)\n",
    "    print(f\"\\r  {label}: [{bar}] {current}/{total}\", end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db5b3d",
   "metadata": {},
   "source": [
    "Main Function\n",
    "- currently keeping small dataset (20 posts + 5 comments each per keyword), might extend later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTS_PER_KEYWORD = 20  # Single control point for limit\n",
    "\n",
    "def main():\n",
    "    print(f\"Starting json scraping script --> Collecting {POSTS_PER_KEYWORD} posts for every keyword with 5 comments each\\n\")\n",
    "\n",
    "    for idx, keyword in enumerate(keywords, 1):\n",
    "        print(f\"Keyword [{idx}/{len(keywords)}]: '{keyword}'\")\n",
    "        posts = search_keyword(keyword)\n",
    "        keyword_count = 0\n",
    "        \n",
    "        for post_item in posts:\n",
    "            if keyword_count >= POSTS_PER_KEYWORD:\n",
    "                break\n",
    "            if post_item.get('kind') != 't3':\n",
    "                continue\n",
    "            post = post_item['data']\n",
    "            post_id = post.get('id', '')\n",
    "            if post_id in seen_posts or not post_id:\n",
    "                continue\n",
    "            seen_posts.add(post_id)\n",
    "            keyword_count += 1\n",
    "            \n",
    "            results.append({\n",
    "                'type': 'post',\n",
    "                'title': post.get('title', ''),\n",
    "                'created_utc': datetime.fromtimestamp(post.get('created_utc', 0)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'body': post.get('selftext', ''),\n",
    "                'num_comments': post.get('num_comments', 0),\n",
    "                'url': f\"{URL}{post.get('permalink', '')}\",\n",
    "                'keyword_matched': keyword,\n",
    "                'parent_post_title': '',\n",
    "            })\n",
    "            \n",
    "            if post.get('num_comments', 0) > 0:\n",
    "                time.sleep(1)\n",
    "                comments = process_comments(get_comments(post_id), post_id, post.get('title', ''))\n",
    "                results.extend(comments)\n",
    "            \n",
    "            print_progress_bar(keyword_count, POSTS_PER_KEYWORD, label=\"Progress\")\n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"ltn_london_reddit_scraped.csv\", index=False, encoding='utf-8')\n",
    "    print(f\"✓ Saved {len(df[df['type'] == 'post'])} posts and {len(df[df['type'] == 'comment'])} comments\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3569d",
   "metadata": {},
   "source": [
    "2. Cleaning Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1fc6f",
   "metadata": {},
   "source": [
    "Use AI  to check if each post is geniunely about LTN \n",
    "\n",
    "- script only saves context relevant post and their comments \n",
    "- small local AI model (llama3.2:3b --> best for my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import time\n",
    "\n",
    "INPUT_FILE = 'ltn_london_reddit_scraped.csv'\n",
    "OUTPUT_FILE = 'filtered_ltn_reddit.csv'\n",
    "MODEL_NAME = 'llama3.2:3b'  # Optimized for 8GB RAM (3x better than 1b)\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"Loaded {len(df)} entries. Filtering with AI...\\n\")\n",
    "\n",
    "\n",
    "def filter_ltn():\n",
    "    filtered = []\n",
    "    for idx, row in df.iterrows():\n",
    "        title = str(row.get('title', '')) if pd.notna(row.get('title')) else \"\"\n",
    "        body = str(row.get('body', '')) if pd.notna(row.get('body')) else \"\"\n",
    "        \n",
    "        prompt = f\"\"\"Is this post about Low Traffic Neighbourhoods (LTN) in London UK?\n",
    "\n",
    "Title: {title}\n",
    "Text: {body[:500]}\n",
    "\n",
    "Answer ONLY 'yes' or 'no'.\"\"\"\n",
    "        \n",
    "        response = ollama.generate(model=MODEL_NAME, prompt=prompt, options={'temperature': 0.2, 'num_predict': 5})\n",
    "        \n",
    "        if 'yes' in response['response'].strip().lower():\n",
    "            filtered.append(row)\n",
    "        \n",
    "        print_progress_bar(idx + 1, len(df), label=\"Filtering\")\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "    return pd.DataFrame(filtered)\n",
    "\n",
    "\n",
    "filtered_df = filter_ltn()\n",
    "filtered_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n\\n✓ Saved {len(filtered_df)} entries to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ae369",
   "metadata": {},
   "source": [
    "3. Sentiment Analysis \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ada24b",
   "metadata": {},
   "source": [
    "Basic sentiment Analysis with Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71da5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 212 entries...\n",
      "\n",
      "\n",
      "Sentiment Distribution:\n",
      "  Positive: 38.7%\n",
      "  Neutral:  17.0%\n",
      "  Negative: 44.3%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df = pd.read_csv('filtered_ltn_reddit.csv')\n",
    "print(f\"Analyzing {len(df)} entries...\\n\")\n",
    "\n",
    "sentiments = []\n",
    "for idx, row in df.iterrows():\n",
    "    text = str(row.get('title', '')) if row['type'] == 'post' else str(row.get('body', ''))\n",
    "    if not text or text == 'nan':\n",
    "        text = str(row.get('body', ''))\n",
    "    \n",
    "    score = analyzer.polarity_scores(text)['compound']\n",
    "    \n",
    "    if score >= 0.05:\n",
    "        sentiments.append('Positive')\n",
    "    elif score <= -0.05:\n",
    "        sentiments.append('Negative')\n",
    "    else:\n",
    "        sentiments.append('Neutral')\n",
    "\n",
    "counts = pd.Series(sentiments).value_counts()\n",
    "total = len(sentiments)\n",
    "print(f\"\\nSentiment Distribution:\\n  Positive: {counts.get('Positive', 0)/total*100:.1f}%\\n  Neutral:  {counts.get('Neutral', 0)/total*100:.1f}%\\n  Negative: {counts.get('Negative', 0)/total*100:.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
