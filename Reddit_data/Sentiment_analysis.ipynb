{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1002fac3",
   "metadata": {},
   "source": [
    "# Reddit Sentiment Anlysis: London low-traffic Neighborhood zones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466cc628",
   "metadata": {},
   "source": [
    "# 1. Data \"Aquisition\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce994ab4",
   "metadata": {},
   "source": [
    "Reddit r/London scraping script\n",
    "- accesses json endpoint \n",
    "- looks for given keywords \n",
    "- gets given number of posts + 5 comments each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d10070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote\n",
    "import json\n",
    "\n",
    "BASE_URL = \"https://www.reddit.com\"\n",
    "HEADERS = {'User-Agent': 'LTN_Sentiment_Analysis (University of Amsterdam)'}\n",
    "\n",
    "keywords = [\n",
    "    \"low traffic neighbourhood\",\n",
    "    \"low-traffic neighbourhood\",\n",
    "    \"LTN\",\n",
    "    \"low-traffic zones\"\n",
    "]\n",
    "\n",
    "POSTS_PER_KEYWORD = 40  # Single control point for limit\n",
    "\n",
    "results = []\n",
    "seen_posts = set()\n",
    "\n",
    "\n",
    "def search_keyword(keyword, retries=3, backoff=2):\n",
    "    url = f\"{BASE_URL}/search.json?q={quote(keyword)}&sort=relevance&limit=100&t=all\"\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            if resp.status_code == 429:\n",
    "                wait = backoff * attempt\n",
    "                print(f\"Rate limited (429). Sleeping {wait}s then retrying...\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            resp.raise_for_status()\n",
    "            try:\n",
    "                data = resp.json()\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Failed to decode JSON from search endpoint; response snippet:\")\n",
    "                print(resp.text[:300])\n",
    "                return []\n",
    "            return data.get('data', {}).get('children', [])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error (attempt {attempt}/{retries}): {e}\")\n",
    "            time.sleep(backoff * attempt)\n",
    "    return []\n",
    "\n",
    "\n",
    "def get_comments(post_id, retries=2):\n",
    "    url = f\"{BASE_URL}/comments/{post_id}.json\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        try:\n",
    "            data = resp.json()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON when fetching comments for {post_id}\")\n",
    "            return []\n",
    "        return data[1].get('data', {}).get('children', []) if len(data) >= 2 else []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request error fetching comments for {post_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def process_comments(comment_data, post_id, post_title, max_comments=5):\n",
    "    comments = []\n",
    "\n",
    "    for item in comment_data:\n",
    "        if len(comments) >= max_comments:\n",
    "            break\n",
    "\n",
    "        if item.get('kind') == 't1':\n",
    "            comment = item['data']\n",
    "            body = comment.get('body', '')\n",
    "\n",
    "            if body and body not in ('[removed]', '[deleted]'):\n",
    "                comments.append({\n",
    "                    'type': 'comment',\n",
    "                    'title': '',\n",
    "                    'created_utc': datetime.fromtimestamp(\n",
    "                        comment.get('created_utc', 0)\n",
    "                    ).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                    'body': body,\n",
    "                    'num_comments': '',\n",
    "                    'url': f\"https://www.reddit.com{comment.get('permalink', '')}\",\n",
    "                    'keyword_matched': '',\n",
    "                    'parent_post_title': post_title,\n",
    "                })\n",
    "\n",
    "            if len(comments) < max_comments:\n",
    "                replies = comment.get('replies', {})\n",
    "                if isinstance(replies, dict):\n",
    "                    children = replies.get('data', {}).get('children', [])\n",
    "                    comments.extend(\n",
    "                        process_comments(\n",
    "                            children,\n",
    "                            post_id,\n",
    "                            post_title,\n",
    "                            max_comments - len(comments),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    return comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d9c288",
   "metadata": {},
   "source": [
    "## Nice Progress Bar by chat gpt :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "564c5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable progress bar function\n",
    "def print_progress_bar(current, total, bar_length=40, label=\"Progress\"):\n",
    "    percent = current / total\n",
    "    filled = int(bar_length * percent)\n",
    "    bar = '█' * filled + '░' * (bar_length - filled)\n",
    "    print(f\"\\r  {label}: [{bar}] {current}/{total}\", end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db5b3d",
   "metadata": {},
   "source": [
    "## Main Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8a4525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting json scraping script --> Collecting 100 posts for every keyword with 5 comments each\n",
      "\n",
      "Keyword [1/4]: 'low traffic neighbourhood'\n",
      "  Progress: [████████████████████████████████████████] 100/100\n",
      "\n",
      "Keyword [2/4]: 'low-traffic neighbourhood'\n",
      "  Progress: [██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 6/100\n",
      "\n",
      "Keyword [3/4]: 'LTN'\n",
      "  Progress: [██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 7/100Request error fetching comments for 1igfx19: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1igfx19.json\n",
      "  Progress: [███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 8/100Request error fetching comments for 1jw265o: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1jw265o.json\n",
      "  Progress: [███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 9/100Request error fetching comments for 1pvonn6: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pvonn6.json\n",
      "  Progress: [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10/100Request error fetching comments for 1nmqnj8: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1nmqnj8.json\n",
      "  Progress: [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 11/100Request error fetching comments for 1ptqho4: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1ptqho4.json\n",
      "  Progress: [████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 12/100Request error fetching comments for 1fnd3r2: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1fnd3r2.json\n",
      "  Progress: [█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 13/100Request error fetching comments for 1jflm30: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1jflm30.json\n",
      "  Progress: [█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 14/100Request error fetching comments for 1oef18y: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1oef18y.json\n",
      "  Progress: [██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15/100Request error fetching comments for 1dhl9kx: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1dhl9kx.json\n",
      "  Progress: [██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 16/100Request error fetching comments for 1cvt51z: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1cvt51z.json\n",
      "  Progress: [██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 17/100Request error fetching comments for 1nb415m: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1nb415m.json\n",
      "  Progress: [███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 18/100Request error fetching comments for 1neam99: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1neam99.json\n",
      "  Progress: [███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 19/100Request error fetching comments for 1pvjcvi: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pvjcvi.json\n",
      "  Progress: [████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20/100Request error fetching comments for 1om0tf1: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1om0tf1.json\n",
      "  Progress: [████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 21/100Request error fetching comments for xho9ua: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/xho9ua.json\n",
      "  Progress: [████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 22/100Request error fetching comments for 1itb50p: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1itb50p.json\n",
      "  Progress: [█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 23/100Request error fetching comments for 1itb3si: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1itb3si.json\n",
      "  Progress: [█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 24/100Request error fetching comments for 1fnf23g: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1fnf23g.json\n",
      "  Progress: [██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25/100Request error fetching comments for 1oqq3ei: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1oqq3ei.json\n",
      "  Progress: [██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 26/100Request error fetching comments for 1pizjof: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pizjof.json\n",
      "  Progress: [██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 27/100Request error fetching comments for 1pu6oq7: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pu6oq7.json\n",
      "  Progress: [███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 28/100Request error fetching comments for 1171uh0: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1171uh0.json\n",
      "  Progress: [███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 29/100Request error fetching comments for 1oso3ft: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1oso3ft.json\n",
      "  Progress: [████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30/100Request error fetching comments for 1oeovxw: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1oeovxw.json\n",
      "  Progress: [████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 31/100Request error fetching comments for 11r9fxn: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/11r9fxn.json\n",
      "  Progress: [████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 32/100Request error fetching comments for 1p1ihl1: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1p1ihl1.json\n",
      "  Progress: [█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 33/100Request error fetching comments for 1pm6qmf: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pm6qmf.json\n",
      "  Progress: [█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 34/100Request error fetching comments for 1kgxyee: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1kgxyee.json\n",
      "  Progress: [██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 35/100Request error fetching comments for 1c3r5o1: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1c3r5o1.json\n",
      "  Progress: [██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 36/100Request error fetching comments for 1eolo01: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1eolo01.json\n",
      "  Progress: [██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 37/100Request error fetching comments for 1ncpbku: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1ncpbku.json\n",
      "  Progress: [███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 38/100Request error fetching comments for 12tu928: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/12tu928.json\n",
      "  Progress: [███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 39/100Request error fetching comments for 1pgulxv: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pgulxv.json\n",
      "  Progress: [████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 40/100Request error fetching comments for 1e26283: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1e26283.json\n",
      "  Progress: [████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 41/100Request error fetching comments for 1p2w8vf: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1p2w8vf.json\n",
      "  Progress: [████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 42/100Request error fetching comments for dtw0he: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/dtw0he.json\n",
      "  Progress: [█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 43/100Request error fetching comments for 13taazx: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/13taazx.json\n",
      "  Progress: [█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 44/100Request error fetching comments for 1pmodft: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pmodft.json\n",
      "  Progress: [██████████████████░░░░░░░░░░░░░░░░░░░░░░] 45/100Request error fetching comments for 1ptychv: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1ptychv.json\n",
      "  Progress: [██████████████████░░░░░░░░░░░░░░░░░░░░░░] 46/100Request error fetching comments for 1mu6vb7: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1mu6vb7.json\n",
      "  Progress: [██████████████████░░░░░░░░░░░░░░░░░░░░░░] 47/100Request error fetching comments for 1nkhmvq: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1nkhmvq.json\n",
      "  Progress: [███████████████████░░░░░░░░░░░░░░░░░░░░░] 48/100Request error fetching comments for 1pn3bw0: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pn3bw0.json\n",
      "  Progress: [███████████████████░░░░░░░░░░░░░░░░░░░░░] 49/100Request error fetching comments for 1popztt: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1popztt.json\n",
      "  Progress: [████████████████████░░░░░░░░░░░░░░░░░░░░] 50/100Request error fetching comments for 1o80tbx: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1o80tbx.json\n",
      "  Progress: [████████████████████░░░░░░░░░░░░░░░░░░░░] 51/100Request error fetching comments for 1nsnhi6: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1nsnhi6.json\n",
      "  Progress: [████████████████████░░░░░░░░░░░░░░░░░░░░] 52/100Request error fetching comments for 1nj2cw1: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1nj2cw1.json\n",
      "  Progress: [█████████████████████░░░░░░░░░░░░░░░░░░░] 53/100Request error fetching comments for 1jiy8nb: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1jiy8nb.json\n",
      "  Progress: [█████████████████████░░░░░░░░░░░░░░░░░░░] 54/100Request error fetching comments for 1b5qb9v: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1b5qb9v.json\n",
      "  Progress: [██████████████████████░░░░░░░░░░░░░░░░░░] 55/100Request error fetching comments for 1pdko9t: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pdko9t.json\n",
      "  Progress: [██████████████████████░░░░░░░░░░░░░░░░░░] 56/100Request error fetching comments for 1m4rpum: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1m4rpum.json\n",
      "  Progress: [██████████████████████░░░░░░░░░░░░░░░░░░] 57/100Request error fetching comments for 1pjdf2y: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pjdf2y.json\n",
      "  Progress: [███████████████████████░░░░░░░░░░░░░░░░░] 58/100Request error fetching comments for 1lxy1r0: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1lxy1r0.json\n",
      "  Progress: [███████████████████████░░░░░░░░░░░░░░░░░] 59/100Request error fetching comments for sihow9: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/sihow9.json\n",
      "  Progress: [████████████████████████░░░░░░░░░░░░░░░░] 60/100Request error fetching comments for 1oqpdcw: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1oqpdcw.json\n",
      "  Progress: [████████████████████████░░░░░░░░░░░░░░░░] 61/100Request error fetching comments for 1p5svp1: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1p5svp1.json\n",
      "  Progress: [████████████████████████░░░░░░░░░░░░░░░░] 62/100Request error fetching comments for 17kkkfh: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/17kkkfh.json\n",
      "  Progress: [█████████████████████████░░░░░░░░░░░░░░░] 63/100Request error fetching comments for 1ijvkqa: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1ijvkqa.json\n",
      "  Progress: [█████████████████████████░░░░░░░░░░░░░░░] 64/100Request error fetching comments for 1ptogns: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1ptogns.json\n",
      "  Progress: [██████████████████████████░░░░░░░░░░░░░░] 65/100Request error fetching comments for 1of0hvn: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1of0hvn.json\n",
      "  Progress: [██████████████████████████░░░░░░░░░░░░░░] 66/100Request error fetching comments for b8zgya: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/b8zgya.json\n",
      "  Progress: [██████████████████████████░░░░░░░░░░░░░░] 67/100Request error fetching comments for 1pmra3a: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pmra3a.json\n",
      "  Progress: [███████████████████████████░░░░░░░░░░░░░] 68/100Request error fetching comments for 1m59llh: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1m59llh.json\n",
      "  Progress: [███████████████████████████░░░░░░░░░░░░░] 69/100Request error fetching comments for ktovm7: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/ktovm7.json\n",
      "  Progress: [████████████████████████████░░░░░░░░░░░░] 70/100Request error fetching comments for uoqe0m: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/uoqe0m.json\n",
      "  Progress: [████████████████████████████░░░░░░░░░░░░] 71/100Request error fetching comments for 1g0g27f: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1g0g27f.json\n",
      "  Progress: [████████████████████████████░░░░░░░░░░░░] 72/100Request error fetching comments for 1dbbf6c: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1dbbf6c.json\n",
      "  Progress: [█████████████████████████████░░░░░░░░░░░] 73/100Request error fetching comments for 1pap2h6: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pap2h6.json\n",
      "  Progress: [█████████████████████████████░░░░░░░░░░░] 74/100Request error fetching comments for 1pth389: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pth389.json\n",
      "  Progress: [██████████████████████████████░░░░░░░░░░] 75/100Request error fetching comments for 1pw4qbo: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pw4qbo.json\n",
      "  Progress: [██████████████████████████████░░░░░░░░░░] 76/100Request error fetching comments for 13x53rm: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/13x53rm.json\n",
      "  Progress: [██████████████████████████████░░░░░░░░░░] 77/100Request error fetching comments for 1o5pje2: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1o5pje2.json\n",
      "  Progress: [███████████████████████████████░░░░░░░░░] 78/100Request error fetching comments for 1nej3qr: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1nej3qr.json\n",
      "  Progress: [███████████████████████████████░░░░░░░░░] 79/100Request error fetching comments for 1p01za1: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1p01za1.json\n",
      "  Progress: [████████████████████████████████░░░░░░░░] 80/100Request error fetching comments for 1di932p: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1di932p.json\n",
      "  Progress: [████████████████████████████████░░░░░░░░] 81/100Request error fetching comments for 1o8s4y0: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1o8s4y0.json\n",
      "  Progress: [████████████████████████████████░░░░░░░░] 82/100Request error fetching comments for 1pg1pb4: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pg1pb4.json\n",
      "  Progress: [█████████████████████████████████░░░░░░░] 83/100Request error fetching comments for 1o551x9: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1o551x9.json\n",
      "  Progress: [█████████████████████████████████░░░░░░░] 84/100Request error fetching comments for 1n5xasx: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1n5xasx.json\n",
      "  Progress: [██████████████████████████████████░░░░░░] 85/100Request error fetching comments for 120r121: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/120r121.json\n",
      "  Progress: [██████████████████████████████████░░░░░░] 86/100Request error fetching comments for 1odrpli: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1odrpli.json\n",
      "  Progress: [██████████████████████████████████░░░░░░] 87/100Request error fetching comments for 1nk5n13: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1nk5n13.json\n",
      "  Progress: [███████████████████████████████████░░░░░] 88/100Request error fetching comments for 172n14z: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/172n14z.json\n",
      "  Progress: [███████████████████████████████████░░░░░] 89/100Request error fetching comments for 1ppw6ve: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1ppw6ve.json\n",
      "  Progress: [████████████████████████████████████░░░░] 90/100Request error fetching comments for 1pcj8oo: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pcj8oo.json\n",
      "  Progress: [████████████████████████████████████░░░░] 91/100Request error fetching comments for 1plzg12: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1plzg12.json\n",
      "  Progress: [████████████████████████████████████░░░░] 92/100Request error fetching comments for 15qu2cz: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/15qu2cz.json\n",
      "  Progress: [█████████████████████████████████████░░░] 93/100Request error fetching comments for 1brudtd: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1brudtd.json\n",
      "  Progress: [█████████████████████████████████████░░░] 94/100Request error fetching comments for 171kmba: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/171kmba.json\n",
      "  Progress: [██████████████████████████████████████░░] 95/100Request error fetching comments for 1pr46gm: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pr46gm.json\n",
      "  Progress: [██████████████████████████████████████░░] 96/100Request error fetching comments for 1pv5klf: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1pv5klf.json\n",
      "  Progress: [██████████████████████████████████████░░] 97/100Request error fetching comments for 1ozwodi: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1ozwodi.json\n",
      "  Progress: [███████████████████████████████████████░] 98/100Request error fetching comments for 1kxf26v: 429 Client Error: Too Many Requests for url: https://www.reddit.com/comments/1kxf26v.json\n",
      "  Progress: [███████████████████████████████████████░] 99/100\n",
      "\n",
      "Keyword [4/4]: 'low-traffic zones'\n",
      "Rate limited (429). Sleeping 2s then retrying...\n",
      "Rate limited (429). Sleeping 4s then retrying...\n",
      "Rate limited (429). Sleeping 6s then retrying...\n",
      "\n",
      "\n",
      "✓ Saved 205 posts and 458 comments\n"
     ]
    }
   ],
   "source": [
    "POSTS_PER_KEYWORD = 100  # Single control point for limit\n",
    "\n",
    "def main():\n",
    "    print(f\"Starting json scraping script --> Collecting {POSTS_PER_KEYWORD} posts for every keyword with 5 comments each\\n\")\n",
    "\n",
    "    for idx, keyword in enumerate(keywords, 1):\n",
    "        print(f\"Keyword [{idx}/{len(keywords)}]: '{keyword}'\")\n",
    "        posts = search_keyword(keyword)\n",
    "        keyword_count = 0\n",
    "        \n",
    "        for post_item in posts:\n",
    "            if keyword_count >= POSTS_PER_KEYWORD:\n",
    "                break\n",
    "            if post_item.get('kind') != 't3':\n",
    "                continue\n",
    "            post = post_item['data']\n",
    "            post_id = post.get('id', '')\n",
    "            if post_id in seen_posts or not post_id:\n",
    "                continue\n",
    "            seen_posts.add(post_id)\n",
    "            keyword_count += 1\n",
    "            \n",
    "            results.append({\n",
    "                'type': 'post',\n",
    "                'title': post.get('title', ''),\n",
    "                'created_utc': datetime.fromtimestamp(post.get('created_utc', 0)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'body': post.get('selftext', ''),\n",
    "                'num_comments': post.get('num_comments', 0),\n",
    "                'url': f\"{URL}{post.get('permalink', '')}\",\n",
    "                'keyword_matched': keyword,\n",
    "                'parent_post_title': '',\n",
    "            })\n",
    "            \n",
    "            if post.get('num_comments', 0) > 0:\n",
    "                time.sleep(1)\n",
    "                comments = process_comments(get_comments(post_id), post_id, post.get('title', ''))\n",
    "                results.extend(comments)\n",
    "            \n",
    "            print_progress_bar(keyword_count, POSTS_PER_KEYWORD, label=\"Progress\")\n",
    "            time.sleep(1)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"ltn_london_reddit_scraped2.csv\", index=False, encoding='utf-8')\n",
    "    print(f\"✓ Saved {len(df[df['type'] == 'post'])} posts and {len(df[df['type'] == 'comment'])} comments\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3569d",
   "metadata": {},
   "source": [
    "# 2. Cleaning Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1fc6f",
   "metadata": {},
   "source": [
    "Use AI  to check if each post is geniunely about LTN \n",
    "\n",
    "- script only saves context relevant post and their comments \n",
    "- small local LLM model (llama3.2:3b --> best for my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c398e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 663 entries. Filtering with AI...\n",
      "\n",
      "  Filtering: [████████████████████████████████████████] 663/663\n",
      "\n",
      "✓ Saved 340 entries to filtered_ltn_reddit2.csv\n",
      "✓ Annotated original file and saved to ltn_london_reddit_scraped2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import time\n",
    "\n",
    "INPUT_FILE = 'ltn_london_reddit_scraped2.csv'\n",
    "OUTPUT_FILE = 'filtered_ltn_reddit2.csv'\n",
    "MODEL_NAME = 'llama3.2:3b'  # Optimized for 8GB RAM with M2 chip\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"Loaded {len(df)} entries. Filtering with AI...\\n\")\n",
    "\n",
    "\n",
    "def filter_ltn():\n",
    "    filtered = []\n",
    "    # initialize Answer column (default 'No')\n",
    "    df['Answer'] = 'No'\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        title = str(row.get('title', '')) if pd.notna(row.get('title')) else \"\"\n",
    "        body = str(row.get('body', '')) if pd.notna(row.get('body')) else \"\"\n",
    "\n",
    "        prompt = f\"\"\"Is this post about Low Traffic Neighbourhoods (LTN) in London UK?\n",
    "\n",
    "Title: {title}\n",
    "Text: {body[:500]}\n",
    "\n",
    "Answer ONLY 'yes' or 'no'.\"\"\"\n",
    "\n",
    "        response = ollama.generate(model=MODEL_NAME, prompt=prompt, options={'temperature': 0.2, 'num_predict': 5})\n",
    "\n",
    "        ans_text = response.get('response', '').strip().lower()\n",
    "        if 'yes' in ans_text:\n",
    "            df.at[idx, 'Answer'] = 'Yes'\n",
    "            filtered.append(row)\n",
    "        else:\n",
    "            df.at[idx, 'Answer'] = 'No'\n",
    "\n",
    "        print_progress_bar(idx + 1, len(df), label=\"Filtering\")\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return pd.DataFrame(filtered)\n",
    "\n",
    "\n",
    "filtered_df = filter_ltn()\n",
    "# Save filtered entries to a new CSV\n",
    "filtered_df.to_csv(OUTPUT_FILE, index=False)\n",
    "# Save annotated original CSV with Answer column\n",
    "df.to_csv(INPUT_FILE, index=False)\n",
    "\n",
    "print(f\"\\n\\n✓ Saved {len(filtered_df)} entries to {OUTPUT_FILE}\")\n",
    "print(f\"✓ Annotated original file and saved to {INPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99eb15a",
   "metadata": {},
   "source": [
    "# Human-LLM Agreement\n",
    "---\n",
    "Checking 50 entries to see if outputs are reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d946ec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported 50 entries to human_ai_validation.csv\n",
      "Answer\n",
      "No     25\n",
      "Yes    25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_TOTAL = 50\n",
    "INPUT_SCRAPED = \"ltn_london_reddit_scraped2.csv\"\n",
    "OUTPUT_FILE = \"human_ai_validation.csv\"\n",
    "\n",
    "df_src = pd.read_csv(INPUT_SCRAPED)\n",
    "\n",
    "df_src['Answer'] = df_src['Answer'].astype(str).str.strip().str.capitalize()\n",
    "labels = sorted(df_src['Answer'].dropna().unique())\n",
    "per_label = (SAMPLE_TOTAL + len(labels) - 1) // len(labels)\n",
    "\n",
    "samples = [\n",
    "    df_src[df_src['Answer'] == lab].sample(min(per_label, len(df_src[df_src['Answer'] == lab])), random_state=42)\n",
    "    for lab in labels\n",
    "]\n",
    "\n",
    "sampled = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "if len(sampled) < SAMPLE_TOTAL:\n",
    "    extra = df_src.drop(sampled.index).sample(SAMPLE_TOTAL - len(sampled), random_state=42)\n",
    "    sampled = pd.concat([sampled, extra]).reset_index(drop=True)\n",
    "\n",
    "# Add human annotation columns and text column\n",
    "sampled['human_label'] = \"\"\n",
    "sampled['notes'] = \"\"\n",
    "if 'text' not in sampled.columns:\n",
    "    sampled['text'] = sampled.get('title', \"\").fillna(\"\").astype(str) + \" \" + sampled.get('body', \"\").fillna(\"\").astype(str)\n",
    "\n",
    "cols_to_save = [c for c in ['created_utc', 'title', 'text', 'body', 'url', 'Answer'] if c in sampled.columns]\n",
    "cols_to_save += ['human_label', 'notes']\n",
    "\n",
    "sampled.to_csv(OUTPUT_FILE, columns=cols_to_save, index=False)\n",
    "print(f\"✓ Exported {len(sampled)} entries to {OUTPUT_FILE}\")\n",
    "print(sampled['Answer'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d8010",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b2911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated rows (yes/no): 50\n",
      "Accuracy (AI Answer == human_label): 92.00%\n",
      "\n",
      "Counts (human_label):\n",
      "human_label\n",
      "yes    29\n",
      "no     21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Confusion (human_label vs AI Answer):\n",
      "Answer       no  yes\n",
      "human_label         \n",
      "no           21    0\n",
      "yes           4   25\n"
     ]
    }
   ],
   "source": [
    "# Simple Yes/No accuracy: compare AI 'Answer' and human 'human_label'\n",
    "df = pd.read_csv(\"human_ai_validation.csv\")\n",
    "\n",
    "# normalize to yes/no lowercase\n",
    "df['Answer'] = df['Answer'].astype(str).str.strip().str.lower()\n",
    "df['human_label'] = df['human_label'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Keep only explicit yes/no human annotations\n",
    "annot = df[df['human_label'].isin(['yes','no'])].copy()\n",
    "print(f\"Annotated rows (yes/no): {len(annot)}\")\n",
    "if len(annot) == 0:\n",
    "    print(\"No human 'yes'/'no' annotations found. Fill 'human_label' with 'Yes' or 'No'.\")\n",
    "else:\n",
    "    matches = (annot['Answer'] == annot['human_label']).astype(int)\n",
    "    acc = matches.mean()\n",
    "    print(f\"Accuracy (AI Answer == human_label): {acc:.2%}\\n\")\n",
    "    print(\"Counts (human_label):\")\n",
    "    print(annot['human_label'].value_counts())\n",
    "    print(\"\\nConfusion (human_label vs AI Answer):\")\n",
    "    print(pd.crosstab(annot['human_label'], annot['Answer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94032e86",
   "metadata": {},
   "source": [
    "# TF-IDF per year\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e3b9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents (months): 47\n",
      "2019 ['grid']\n",
      "2020 ['general', 'islington', 'trial', 'mainly', 'quintiles', 'farage']\n",
      "2021 ['dual', 'encourage', 'dft', 'gent']\n",
      "2022 ['care', 'anonymity', 'warrior', 'hanover', 'pollution']\n",
      "2023 ['making', 'protest', 'famously', 'bcc', 'tutorial', 'evidence', 'sunak', 'fixing', 'appears']\n",
      "2024 ['battle', 'report', 'heaton', 'amenity', 'exeter', 'sign', 'nh', 'pcn', 'forced', 'iron']\n",
      "2025 ['furious', 'leith', 'affluent', 'pushing', 'train', 'highway', 'injury', 'single', 'pedestrian', 'wagon', 'restriction', 'sub']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# load the data\n",
    "df = pd.read_csv(\"filtered_ltn_reddit2.csv\")\n",
    "\n",
    "# Parse datetime (your format is already standard)\n",
    "df[\"created_utc\"] = pd.to_datetime(df[\"created_utc\"])\n",
    "\n",
    "# Combine title + body\n",
    "df[\"text\"] = df[\"title\"].astype(str) + \" \" + df[\"body\"].astype(str)\n",
    "\n",
    "# group by month\n",
    "df[\"month\"] = df[\"created_utc\"].dt.to_period(\"M\").astype(str) + \"-01\"\n",
    "\n",
    "grouped = (\n",
    "    df.groupby(\"month\")[\"text\"]\n",
    "    .apply(\" \".join)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Documents (months): {len(grouped)}\")\n",
    "\n",
    "#text processing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def tokenization(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "grouped[\"tokens\"] = (\n",
    "    grouped[\"text\"]\n",
    "    .apply(clean_text)\n",
    "    .apply(tokenization)\n",
    "    .apply(remove_stopwords)\n",
    "    .apply(lemmatize)\n",
    ")\n",
    "\n",
    "# TF-IDF (manual,per month)\n",
    "def compute_tf_idf_manual(docs):\n",
    "    \"\"\"\n",
    "    docs: list of token lists (one per month)\n",
    "    \"\"\"\n",
    "    N = len(docs)\n",
    "    df_counts = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        for term in set(doc):\n",
    "            df_counts[term] += 1\n",
    "\n",
    "    tfidf_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        tf = Counter(doc)\n",
    "        doc_len = len(doc)\n",
    "        tfidf = {}\n",
    "\n",
    "        for term, count in tf.items():\n",
    "            tf_val = count / doc_len\n",
    "            idf_val = log(N / (1 + df_counts[term]))\n",
    "            tfidf[term] = tf_val * idf_val\n",
    "\n",
    "        tfidf_docs.append(tfidf)\n",
    "\n",
    "    return tfidf_docs\n",
    "\n",
    "grouped[\"tfidf\"] = compute_tf_idf_manual(grouped[\"tokens\"].tolist())\n",
    "\n",
    "# top terms per month\n",
    "def get_highest_values(tfidf_dict, n=5):\n",
    "    return sorted(tfidf_dict.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "grouped[\"tfidf_top\"] = grouped[\"tfidf\"].apply(get_highest_values)\n",
    "\n",
    "# print year by year\n",
    "grouped[\"year\"] = grouped[\"month\"].str[:4]\n",
    "\n",
    "for year in grouped[\"year\"].unique():\n",
    "    gy = grouped[grouped[\"year\"] == year]\n",
    "    print(\n",
    "        year,\n",
    "        [terms[0][0] for terms in gy[\"tfidf_top\"]]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ae369",
   "metadata": {},
   "source": [
    "# 3. Sentiment Analysis \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ada24b",
   "metadata": {},
   "source": [
    "Basic sentiment Analysis with Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71da5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 340 entries...\n",
      "\n",
      "\n",
      "Sentiment Distribution:\n",
      "  Positive: 41.2%\n",
      "  Neutral:  18.2%\n",
      "  Negative: 40.6%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Please install transformers and torch: pip install transformers torch\") from e\n",
    "\n",
    "# load the data\n",
    "df = pd.read_csv('filtered_ltn_reddit2.csv')\n",
    "print(f\"Analyzing {len(df)} entries...\\n\")\n",
    "\n",
    "# Create a sentiment pipeline using the CardiffNLP model\n",
    "nlp = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "# Prepare texts (match Vader logic: title for posts, body for comments)\n",
    "texts = []\n",
    "for _, row in df.iterrows():\n",
    "    if row.get('type') == 'post':\n",
    "        text = str(row.get('title', ''))\n",
    "    else:\n",
    "        text = str(row.get('body', ''))\n",
    "    if not text or text == 'nan':\n",
    "        text = str(row.get('body', ''))\n",
    "    texts.append(text)\n",
    "\n",
    "# Run in batches for efficiency\n",
    "results = nlp(texts, truncation=True, batch_size=32)\n",
    "\n",
    "sentiment_scores = []\n",
    "sentiment_labels = []\n",
    "for res in results:\n",
    "    # res is a list of dicts like [{'label':'NEGATIVE','score':0.9}, ...]\n",
    "    if isinstance(res, dict):\n",
    "        # unexpected single dict — normalize\n",
    "        res_list = [res]\n",
    "    else:\n",
    "        res_list = res\n",
    "    score_map = {d['label'].lower(): d['score'] for d in res_list}\n",
    "    pos = score_map.get('positive', 0.0)\n",
    "    neu = score_map.get('neutral', 0.0)\n",
    "    neg = score_map.get('negative', 0.0)\n",
    "\n",
    "    # sentiment_score: positive minus negative (approx -1..1)\n",
    "    sentiment_scores.append(pos - neg)\n",
    "\n",
    "    # label by highest probability\n",
    "    if pos >= neg and pos >= neu:\n",
    "        sentiment_labels.append('Positive')\n",
    "    elif neg >= pos and neg >= neu:\n",
    "        sentiment_labels.append('Negative')\n",
    "    else:\n",
    "        sentiment_labels.append('Neutral')\n",
    "\n",
    "# Save results to dataframe\n",
    "df['sentiment_score'] = sentiment_scores\n",
    "df['sentiment_label'] = sentiment_labels\n",
    "\n",
    "# print distribution\n",
    "counts = df['sentiment_label'].value_counts()\n",
    "total = len(df)\n",
    "\n",
    "print(\n",
    "    f\"\\nSentiment Distribution:\\n\"\n",
    "    f\"  Positive: {counts.get('Positive', 0)/total*100:.1f}%\\n\"\n",
    "    f\"  Neutral:  {counts.get('Neutral', 0)/total*100:.1f}%\\n\"\n",
    "    f\"  Negative: {counts.get('Negative', 0)/total*100:.1f}%\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562e60c",
   "metadata": {},
   "source": [
    "# 4. Inter-Annotator Agreement\n",
    "---\n",
    "Checking whether these outputs are reliable by looking at 60 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48c322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Exported sentiment_human_validation.csv for manual annotation\n"
     ]
    }
   ],
   "source": [
    "# this makes a file in which I can fill in the human_label (so whether I find the entry positive, negative or neutral)\n",
    "SAMPLE_SIZE = 20  # per class\n",
    "\n",
    "sampled = pd.concat([\n",
    "    df[df['sentiment_label'] == 'Positive'].sample(SAMPLE_SIZE, random_state=42),\n",
    "    df[df['sentiment_label'] == 'Neutral'].sample(SAMPLE_SIZE, random_state=42),\n",
    "    df[df['sentiment_label'] == 'Negative'].sample(SAMPLE_SIZE, random_state=42),\n",
    "])\n",
    "\n",
    "# Columns for human annotation\n",
    "sampled['human_label'] = \"\"\n",
    "sampled['notes'] = \"\"\n",
    "\n",
    "# Combine text for easier reading\n",
    "sampled['text'] = (\n",
    "    sampled['title'].fillna(\"\").astype(str) + \" \" +\n",
    "    sampled['body'].fillna(\"\").astype(str)\n",
    ")\n",
    "\n",
    "# Export for manual checking\n",
    "sampled[[\n",
    "    'created_utc',\n",
    "    'text',\n",
    "    'sentiment_score',\n",
    "    'sentiment_label',\n",
    "    'human_label'\n",
    "]].to_csv('sentiment_human_validation.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Exported sentiment_human_validation.csv for manual annotation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac82e22",
   "metadata": {},
   "source": [
    "# Check the Human–VADER agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92572121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['created_utc', 'text', 'sentiment_score', 'sentiment_label', 'human_label']\n",
      "Human–VADER agreement: 65.00%\n",
      "sentiment_label  Negative   Neutral  Positive\n",
      "human_label                                  \n",
      "Negative         0.666667  0.166667  0.166667\n",
      "Neutral          0.291667  0.625000  0.083333\n",
      "Positive         0.208333  0.125000  0.666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV\n",
    "val = pd.read_csv(\"sentiment_human_validation.csv\", sep=',', on_bad_lines='skip')\n",
    "\n",
    "# Strip any whitespace from column names\n",
    "val.columns = val.columns.str.strip()\n",
    "\n",
    "# Check the column names\n",
    "print(val.columns.tolist())\n",
    "\n",
    "# Now calculate agreement\n",
    "accuracy = (val[\"sentiment_label\"] == val[\"human_label\"]).mean()\n",
    "print(f\"Human–VADER agreement: {accuracy:.2%}\")\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = pd.crosstab(\n",
    "    val[\"human_label\"],\n",
    "    val[\"sentiment_label\"],\n",
    "    normalize=\"index\"\n",
    ")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f6266",
   "metadata": {},
   "source": [
    "Results:\n",
    "#\n",
    "Here we assume the human is the correct answer.\n",
    "- Negative was correct 66% of the time\n",
    "- Neutral was correct 62% of the time\n",
    "- Positive was correct 66% of the time\n",
    "#\n",
    "Interpretation\n",
    "- Vader is biased towards detecting negative sentiment, it over-predicts Negative for neutral posts.\n",
    "- Neutral posts are hardest for Vader.\n",
    "- Vader performs reasonably well for strongly polarized posts, but struggles to identify neutral or mixed sentiment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfd048",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with pre-trained transformer based model \n",
    "---\n",
    "- https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c24ccde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 340 entries...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved annotated filtered file with Cardiff labels to filtered_ltn_reddit2.csv\n",
      "\n",
      "Sentiment Distribution (CardiffNLP):\n",
      "  Positive: 12.4%\n",
      "  Neutral:  43.5%\n",
      "  Negative: 44.1%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Please install transformers and torch: pip install transformers torch\") from e\n",
    "\n",
    "# load the filtered data\n",
    "df = pd.read_csv(\"filtered_ltn_reddit2.csv\")\n",
    "print(f\"Analyzing {len(df)} entries...\\n\")\n",
    "\n",
    "# Create a sentiment pipeline using the CardiffNLP model\n",
    "nlp = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    top_k=None,  # return scores for all labels\n",
    ")\n",
    "\n",
    "# prepare texts (title for posts, body for comments)\n",
    "texts = []\n",
    "for _, row in df.iterrows():\n",
    "    if row.get('type') == 'post':\n",
    "        text = str(row.get('title', ''))\n",
    "    else:\n",
    "        text = str(row.get('body', ''))\n",
    "    if not text or text == 'nan':\n",
    "        text = str(row.get('body', ''))\n",
    "    texts.append(text)\n",
    "\n",
    "# Run in batches for efficiency; specify max_length to avoid truncation warning\n",
    "results = nlp(texts, truncation=True, max_length=128, batch_size=32, top_k=None)\n",
    "\n",
    "sentiment_scores = []\n",
    "sentiment_labels = []\n",
    "for res in results:\n",
    "    # res is a list of dicts like [{'label':'NEGATIVE','score':0.9}, ...]\n",
    "    if isinstance(res, dict):\n",
    "        # unexpected single dict — normalize\n",
    "        res_list = [res]\n",
    "    else:\n",
    "        res_list = res\n",
    "    score_map = {d['label'].lower(): d['score'] for d in res_list}\n",
    "    pos = score_map.get('positive', 0.0)\n",
    "    neu = score_map.get('neutral', 0.0)\n",
    "    neg = score_map.get('negative', 0.0)\n",
    "\n",
    "    # sentiment_score: positive minus negative (approx -1..1)\n",
    "    sentiment_scores.append(pos - neg)\n",
    "\n",
    "    # label by highest probability\n",
    "    if pos >= neg and pos >= neu:\n",
    "        sentiment_labels.append('Positive')\n",
    "    elif neg >= pos and neg >= neu:\n",
    "        sentiment_labels.append('Negative')\n",
    "    else:\n",
    "        sentiment_labels.append('Neutral')\n",
    "\n",
    "# Save results to dataframe\n",
    "df['sentiment_score_cardiff'] = sentiment_scores\n",
    "df['sentiment_label_cardiff'] = sentiment_labels\n",
    "\n",
    "# Save annotated filtered CSV so agreement cells can use it\n",
    "df.to_csv(\"filtered_ltn_reddit2.csv\", index=False)\n",
    "print(f'✓ Saved annotated filtered file with Cardiff labels to filtered_ltn_reddit2.csv')\n",
    "\n",
    "# print distribution (like the Vader cell did)\n",
    "counts = df['sentiment_label_cardiff'].value_counts()\n",
    "total = len(df)\n",
    "\n",
    "print(\n",
    "    f\"\\nSentiment Distribution (CardiffNLP):\\n\" +\n",
    "    f\"  Positive: {counts.get('Positive', 0)/total*100:.1f}%\\n\" +\n",
    "    f\"  Neutral:  {counts.get('Neutral', 0)/total*100:.1f}%\\n\" +\n",
    "    f\"  Negative: {counts.get('Negative', 0)/total*100:.1f}%\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376602a",
   "metadata": {},
   "source": [
    "## Human-Transformer Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ee234ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Exported 60 examples to sentiment_transformer_human_validation.csv (up to 20 per label)\n"
     ]
    }
   ],
   "source": [
    "# Export a balanced sample (20 per Cardiff label) for human annotation\n",
    "import pandas as pd\n",
    "SAMPLE_PER_LABEL = 20\n",
    "INPUT_FILE = 'filtered_ltn_reddit2.csv'\n",
    "OUTPUT_FILE = 'sentiment_transformer_human_validation.csv'\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "# Ensure the transformer label column exists\n",
    "if 'sentiment_label_cardiff' not in df.columns:\n",
    "    raise KeyError('sentiment_label_cardiff not found in filtered_ltn_reddit2.csv — run the Cardiff cell first')\n",
    "\n",
    "labels = ['Positive', 'Neutral', 'Negative']\n",
    "samples = []\n",
    "for lab in labels:\n",
    "    subset = df[df['sentiment_label_cardiff'] == lab]\n",
    "    if len(subset) == 0:\n",
    "        # skip if no examples for this label\n",
    "        continue\n",
    "    n = min(SAMPLE_PER_LABEL, len(subset))\n",
    "    samples.append(subset.sample(n, random_state=42))\n",
    "\n",
    "if len(samples) == 0:\n",
    "    raise ValueError('No examples found for any label — check sentiment_label_cardiff values')\n",
    "\n",
    "sampled = pd.concat(samples).reset_index(drop=True)\n",
    "target_total = SAMPLE_PER_LABEL * len(labels)\n",
    "if len(sampled) < target_total:\n",
    "    # add extra random samples from remaining pool to reach target size\n",
    "    remaining = df.drop(sampled.index, errors='ignore')\n",
    "    need = target_total - len(sampled)\n",
    "    if len(remaining) > 0 and need > 0:\n",
    "        extra = remaining.sample(min(need, len(remaining)), random_state=42)\n",
    "        sampled = pd.concat([sampled, extra]).reset_index(drop=True)\n",
    "\n",
    "# Add human annotation placeholders\n",
    "sampled['human_label'] = ''\n",
    "sampled['notes'] = ''\n",
    "# create a combined text field for annotation convenience\n",
    "sampled['text'] = sampled.get('title', '').fillna('').astype(str) + ' ' + sampled.get('body', '').fillna('').astype(str)\n",
    "\n",
    "cols = [c for c in ['created_utc', 'title', 'text', 'sentiment_score_cardiff', 'sentiment_label_cardiff', 'url', 'keyword_matched'] if c in sampled.columns]\n",
    "cols += ['human_label', 'notes']\n",
    "sampled.to_csv(OUTPUT_FILE, columns=cols, index=False)\n",
    "print(f'✓ Exported {len(sampled)} examples to {OUTPUT_FILE} (up to {SAMPLE_PER_LABEL} per label)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb7e84a",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c7a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['created_utc', 'title', 'text', 'sentiment_score_cardiff', 'sentiment_label_cardiff', 'url', 'keyword_matched', 'human_label', 'notes']\n",
      "Human–Transformer agreement: 73.33%\n",
      "sentiment_label_cardiff  Negative   Neutral  Positive\n",
      "human_label                                          \n",
      "Negative                 1.000000  0.000000  0.000000\n",
      "Neutral                  0.173913  0.695652  0.130435\n",
      "Positive                 0.192308  0.153846  0.653846\n"
     ]
    }
   ],
   "source": [
    "# Read CSV with semicolon separator\n",
    "val = pd.read_csv(\"sentiment_transformer_human_validation.csv\", sep=',', on_bad_lines='skip')\n",
    "\n",
    "# Strip any whitespace from column names\n",
    "val.columns = val.columns.str.strip()\n",
    "\n",
    "# Check the column names\n",
    "print(val.columns.tolist())\n",
    "\n",
    "# Now calculate agreement\n",
    "accuracy = (val[\"sentiment_label_cardiff\"] == val[\"human_label\"]).mean()\n",
    "print(f\"Human–Transformer agreement: {accuracy:.2%}\")\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = pd.crosstab(\n",
    "    val[\"human_label\"],\n",
    "    val[\"sentiment_label_cardiff\"],\n",
    "    normalize=\"index\"\n",
    ")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31ccf9",
   "metadata": {},
   "source": [
    "- higher Human-transformer agreement 73%\n",
    "- all human negative labels were actually machine negative labels as well --> more accurate for negative posts\n",
    "- Neutral matches 70%, positive 65% \n",
    "--> performs well but sometimes confuses these two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca7b43",
   "metadata": {},
   "source": [
    "--> More reliable than Vader --> using Transformer Sentiment for Website but calculating neutrals as positives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7975c0",
   "metadata": {},
   "source": [
    "# Add to Database for Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5df37c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sentiment data saved to sentiment_data.db\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Initialize database and create table\n",
    "conn = sqlite3.connect('sentiment_data.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS sentiment_distribution (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    sentiment_type TEXT NOT NULL,\n",
    "    count INTEGER NOT NULL,\n",
    "    percentage REAL NOT NULL,\n",
    "    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "''')\n",
    "\n",
    "# Get counts from your dataframe\n",
    "counts = df['sentiment_label_cardiff'].value_counts()\n",
    "total = len(df)\n",
    "\n",
    "positive_count = counts.get('Positive', 0)\n",
    "neutral_count = counts.get('Neutral', 0)\n",
    "negative_count = counts.get('Negative', 0)\n",
    "\n",
    "# Insert data\n",
    "cursor.execute('INSERT INTO sentiment_distribution (sentiment_type, count, percentage) VALUES (?, ?, ?)',\n",
    "               ('Positive', int(positive_count), positive_count/total*100))\n",
    "cursor.execute('INSERT INTO sentiment_distribution (sentiment_type, count, percentage) VALUES (?, ?, ?)',\n",
    "               ('Neutral', int(neutral_count), neutral_count/total*100))\n",
    "cursor.execute('INSERT INTO sentiment_distribution (sentiment_type, count, percentage) VALUES (?, ?, ?)',\n",
    "               ('Negative', int(negative_count), negative_count/total*100))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"✓ Sentiment data saved to sentiment_data.db\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
